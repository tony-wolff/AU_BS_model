{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "this code is used to create the dataset for the machine learning model au-bs model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy import interpolate\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "path = \"C:/Users/Tony/Documents/TestData/\"\n",
    "list_participants_id_path = [f.path for f in os.scandir(path) if f.is_dir() and \".\" not in f.name]\n",
    "#list_of_openface_csvs = [f.path for f in os.scandir(participant_path) if f.is_file and f.name.endswith('.csv')]\n",
    "\n",
    "def low_pass_filter(csv_file: str, window_ms: int=100) -> pd.DataFrame:\n",
    "    '''\n",
    "    low pass filter the data to filter noise using a sliding windows of 100ms by default\n",
    "    outputs a new csv file\n",
    "    '''\n",
    "    window_size = int(window_ms*60/1000)\n",
    "    bs_df = pd.read_csv(csv_file, sep=', ')\n",
    "    #drop the frame columnm, as it is not useful anymore\n",
    "    rolling = bs_df.drop(\"frame\", axis=1).rolling(window_size).mean()\n",
    "    return rolling\n",
    "    \n",
    "def rreplace(s, old, new, occurence):\n",
    "    '''\n",
    "    find 'old' and replace by 'new'\n",
    "    it does so 'occurrence' times\n",
    "    starting at the end of 's'\n",
    "    '''\n",
    "    li = s.rsplit(old, occurence)\n",
    "    return new.join(li)\n",
    "\n",
    "def processing_milliseconds(s):\n",
    "    '''\n",
    "    converts milliseconds base 60 into actual milliseconds base 10\n",
    "    does not work with negative time\n",
    "    '''\n",
    "    li = s.rsplit(':', 1)\n",
    "    millisec = float(li[1])\n",
    "    #divide by 60 and multiply by 1sec in microseconds\n",
    "    millisec = (millisec/60)*1e6\n",
    "    if len(str(int(millisec))) < 6:\n",
    "        li[1] = ':0'+str(millisec)\n",
    "    else:\n",
    "        li[1] = ':'+str(millisec) \n",
    "    return ''.join(li)\n",
    "\n",
    "def compute_framerate(data):\n",
    "    n_seconds = np.sum(np.diff(data))\n",
    "    n_frames = data.shape[0]-1\n",
    "    framerate = n_frames / n_seconds\n",
    "    return framerate\n",
    "\n",
    "def find_corresponding_blendshape_csv(participants_id_path, filepath):\n",
    "    filename = os.path.basename(filepath)\n",
    "    path_to_bs_csv = participants_id_path+\"/csv_whole/individual\"\n",
    "    emotion_intensity = filename.split('!')[1]\n",
    "    emotion_number = filename.split('!')[2]\n",
    "    file = [f.path for f in os.scandir(path_to_bs_csv) if emotion_intensity in f.name and emotion_number in f.name]\n",
    "    return file[0]\n",
    "\n",
    "def cross_corr_with_savgol_filter(signal1, signal2, window_size):\n",
    "    '''\n",
    "    returns the correlation and the lags in a tuple\n",
    "    '''\n",
    "    #Before cross correlation, a savgol filter is applied to smooth high frequencies\n",
    "    y_au_filtered = signal.savgol_filter(signal1, window_size, 1)\n",
    "    y_bs_filtered = signal.savgol_filter(signal2.to_numpy().flatten(), window_size, 1)\n",
    "    correlation = signal.correlate(y_au_filtered-np.mean(y_au_filtered), y_bs_filtered-np.mean(y_bs_filtered), mode=\"full\") #substracting the mean makes computing more accurate\n",
    "    #The lag is the refers to how far the series are offset\n",
    "    lags = signal.correlation_lags(len(y_au_filtered), len(y_bs_filtered), mode=\"full\")\n",
    "    return [correlation, lags]\n",
    "\n",
    "def plot_for_testing_purposes(blendshape_file, au_file, au_name, bs_name):\n",
    "    \"\"\"\n",
    "    Plot an action unit and blendshape with specified path of both files\n",
    "    Used to plot often AU26_r and JawOpen\n",
    "    \"\"\"\n",
    "    action_unit = pd.read_csv(au_file, sep=', ', engine=\"python\")\n",
    "    blendshape = pd.read_csv(blendshape_file, sep=',')\n",
    "    x_au = action_unit[\"timestamp\"]\n",
    "    y_au = action_unit[au_name]\n",
    "    x_bs = blendshape[\"Timecode\"]\n",
    "    y_bs = blendshape[bs_name]\n",
    "    y_au/=5 # normalization between 0 and 1\n",
    "\n",
    "    # 21:52:40:24.937 --> 21:52:40:024937\n",
    "    # x2 = x2.apply(processing_milliseconds)\n",
    "    # x2 = x2.str.split('.').str[0]\n",
    "    # 2022-01-01 14:38:12.133575\n",
    "    x_bs = pd.to_datetime(x_bs, format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    x_bs -= x_bs[0]\n",
    "\n",
    "    #Low pass filter of a windows size of 100\n",
    "    y1_tmp = y_au.rolling(6).mean().dropna()\n",
    "    y2_tmp = y_bs.rolling(6).mean().dropna()\n",
    "\n",
    "    y_au = y_au.rolling(6).mean().fillna(np.mean(y1_tmp))\n",
    "    y_bs = y_bs.rolling(6).mean().fillna(np.mean(y2_tmp))\n",
    "\n",
    "    #y_au = y_au.iloc[:len(y_bs)].reset_index(drop=True)\n",
    "\n",
    "    #--Cross correlation--\n",
    "    correlation, lags = cross_corr_with_savgol_filter(y_au, y_bs, 20)\n",
    "    #We get the lag at the peak of the correlation, when both signal correlate the best\n",
    "    lag = lags[np.argmax(abs(correlation))]\n",
    "    #Plottting the signal by slicing values. The number of sliced values is \"lag\"\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig2, ax2 = plt.subplots()\n",
    "    #y_bs = y_bs.iloc[abs(lag):].reset_index(drop=True)\n",
    "    y_bs_resampled=signal.resample(y_bs, 100)\n",
    "    y_au_resampled=signal.resample(y_au, 100)\n",
    "    ax1.plot(y_au, color=\"green\", label=au_name)\n",
    "    ax1.plot(y_bs, color=\"blue\", label=bs_name)\n",
    "    ax2.plot(y_au_resampled, label=au_name + 'Resampled')\n",
    "    ax2.plot(y_bs_resampled, label=bs_name + 'Resampled')\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.title.set_text(Path(blendshape_file).name)\n",
    "    ax2.legend(loc=\"upper right\")\n",
    "    ax2.title.set_text(Path(blendshape_file).name)\n",
    "    #display(fig)\n",
    "    #plt.savefig(\"resampled/\" + Path(blendshape_file).name + \".png\")\n",
    "    \n",
    "def find_all_bs_csvs(row):\n",
    "    participant_id = row.split(\"_\")[0]\n",
    "    folders_in_between = \"/csv_whole/individual/\"\n",
    "    suffix = \"_whole.csv\"\n",
    "    return os.path.join(path, participant_id + folders_in_between + row + suffix)\n",
    "\n",
    "\n",
    "def find_outliers(threshold):\n",
    "    \"\"\"\n",
    "    find the files that have a frame difference over the threshold variable\n",
    "    stores them in a csv\n",
    "    \"\"\"\n",
    "    lag_csv = pd.read_csv(path+ \"/AU26_r-JawOpen-lagMeasure.csv\" , sep=',')\n",
    "    all_bs_csvs = lag_csv[\"blendshape_scriptID\"].apply(find_all_bs_csvs).values.tolist()\n",
    "    all_participants = lag_csv[\"participant_id\"].drop_duplicates(keep=\"first\")\n",
    "    all_au_csvs = all_participants.apply(lambda x: [f.path for f in os.scandir(x) if f.is_file and f.name.endswith('.csv')]) #find all au csv for each folder named as the participants\n",
    "    all_au_csvs = all_au_csvs.explode().reset_index(drop=True).values.tolist() #explode the lists inside cells in rows\n",
    "    frame_diff = lag_csv[\"n_frames_OF\"] - lag_csv[\"n_frame_ARKIT\"]\n",
    "\n",
    "    #outlier threshold: if the frame difference between both file is more than 60 frames, then we select its index\n",
    "    list_frame_list_sup_1s = frame_diff[abs(frame_diff) > threshold].index.values.tolist()\n",
    "    framerate_au = lag_csv[\"average_frame_rate_OF\"].values.tolist()\n",
    "    framerate_bs = lag_csv[\"average_frame_rate_ARKIT\"].values.tolist()\n",
    "    bs_csvs = [all_bs_csvs[index] for index in list_frame_list_sup_1s]\n",
    "    au_csvs = [all_au_csvs[index] for index in list_frame_list_sup_1s]\n",
    "    framerate_au = [framerate_au[index] for index in list_frame_list_sup_1s]\n",
    "    framerate_bs = [framerate_bs[index] for index in list_frame_list_sup_1s]\n",
    "    outlier_frame = pd.DataFrame(columns=[\"index_in_lagMeasure_csv\", \"frame_diff_(au-bs)\", \"openface_framerate\", \"arkit_framerate\",\"action_unit_csv\", \"blend_shape_csv\"])\n",
    "    for au, bs, idx, f_au, f_bs in zip(au_csvs, bs_csvs, list_frame_list_sup_1s, framerate_au, framerate_bs):\n",
    "        au_csv = pd.read_csv(au)\n",
    "        bs_csv = pd.read_csv(bs)\n",
    "        frame_diff = au_csv.shape[0] - bs_csv.shape[0]\n",
    "        dictionary = {\"index_in_lagMeasure_csv\": idx, \"frame_diff_(au-bs)\": frame_diff, \"openface_framerate\": f_au, \"arkit_framerate\": f_bs ,\"action_unit_csv\": Path(au).name, \"blend_shape_csv\": Path(bs).name}\n",
    "        outlier_frame = pd.concat([outlier_frame, pd.DataFrame.from_records([dictionary])], ignore_index=True)\n",
    "    outlier_frame.to_csv(\"outliers.csv\", index=False)\n",
    "\n",
    "\n",
    "def low_pass_filter_and_lag_slicing(bs_dt, au_dt, lag, window_size):\n",
    "    \"\"\"\n",
    "    Apply a low pass filter of size \"window\" on both dataframe au_dt and bs_dt\n",
    "    Slice the lag of the bs dataset, computed earlier and available in the lagMeasure.csv\n",
    "    cut the dataframes so that both files have the same number lines\n",
    "    \"\"\"\n",
    "    #####Low pass filter\n",
    "    #low pass without the nan values\n",
    "    au_tmp = au_dt.rolling(window_size).mean().dropna()\n",
    "    bs_tmp = bs_dt.rolling(window_size).mean().dropna()\n",
    "    #low pass filter + replacing the nan values by the mean of tmp values\n",
    "    au_dt = au_dt.rolling(window_size).mean().fillna(np.mean(au_tmp))\n",
    "    bs_dt = bs_dt.rolling(window_size).mean().fillna(np.mean(bs_tmp))\n",
    "    #Slice the lag at the start of the bs file\n",
    "    bs_dt = bs_dt.iloc[abs(lag):].reset_index(drop=True)\n",
    "\n",
    "    #Slice the end of files so that both files math in length\n",
    "    if au_dt.shape[0] > bs_dt.shape[0]: #if the number of lines of au file is greater than the bs file\n",
    "        au_dt = au_dt.iloc[: bs_dt.shape[0]]\n",
    "    else:\n",
    "        bs_dt = bs_dt.iloc[: au_dt.shape[0]]\n",
    "    return [au_dt, bs_dt]\n",
    "\n",
    "def remove_outliers(list_val, lag_csv, threshold):\n",
    "    '''\n",
    "    remove outliers from the list \"list_val\" where the frame difference between au and blendshape is > to \"threshold\"\n",
    "    '''\n",
    "    frame_diff = lag_csv[\"n_frames_OF\"] - lag_csv[\"n_frame_ARKIT\"]\n",
    "    list_frame_list_sup_1s = frame_diff[abs(frame_diff) <= threshold].index.values.tolist()\n",
    "    return [list_val[index] for index in list_frame_list_sup_1s]\n",
    "\n",
    "def create_au_bs_dataset():\n",
    "    \"\"\"\n",
    "    this methods merge all the data from blendshapes, to action units and other data extracted with OpenFace that are availabe in the au.csv (gaze, 3D landmarks etc.)\n",
    "    It creates a new csv file that will be the foundation for the au to bs machine learning model\n",
    "    the dataset is called giga_dataset\n",
    "    WARNING: the computation takes time\n",
    "    \"\"\"\n",
    "    #Open the csv file\n",
    "    lag_csv = pd.read_csv(path+ \"/AU26_r-JawOpen-lagMeasure.csv\" , sep=',')\n",
    "    all_bs_csvs = lag_csv[\"blendshape_scriptID\"].apply(find_all_bs_csvs).values.tolist()\n",
    "    lag_vals = lag_csv[\"max_lag\"].values.tolist()\n",
    "    #load all the csvs\n",
    "    all_participants = lag_csv[\"participant_id\"].drop_duplicates(keep=\"first\")\n",
    "    all_au_csvs = all_participants.apply(lambda x: [f.path for f in os.scandir(x) if f.is_file and f.name.endswith('.csv')]) #find all au csv for each folder named as the participants\n",
    "    all_au_csvs = all_au_csvs.explode().reset_index(drop=True).values.tolist() #explode the lists inside cells in rows\n",
    "\n",
    "    au_csvs = remove_outliers(all_au_csvs, lag_csv, 60)\n",
    "    bs_csvs = remove_outliers(all_bs_csvs, lag_csv, 60)\n",
    "    lag_vals_without_outliers = remove_outliers(lag_vals, lag_csv, 60)\n",
    "    #print('\\n'.join('{}: {}'.format(*k) for k in enumerate(au_csvs))) #print the data\n",
    "    giga_dataset = pd.DataFrame()\n",
    "    list_of_df = []\n",
    "    for au_path, bs_path, lag in zip(au_csvs, bs_csvs, lag_vals_without_outliers):\n",
    "        bs_dt = pd.read_csv(bs_path, index_col=0) #index_col=0 removes the unnamed: 0 column\n",
    "        au_dt = pd.read_csv(au_path, sep=\", \", engine='python')\n",
    "        #drop unecessary columns\n",
    "        au_dt.drop([\"frame\", \"face_id\", \"timestamp\", \"confidence\", \"success\"], axis=1, inplace=True)\n",
    "        au_dt = au_dt.loc[:, ~au_dt.columns.str.endswith(\"_c\")]\n",
    "        bs_dt = bs_dt.drop([\"Timecode\", \"BlendShapeCount\"], axis=1).reset_index(drop=True) #index starts at 0 with reset_index(drop=True)\n",
    "        #divide by 5 to normalize au value between 0 and 1\n",
    "        au_dt.loc[:, au_dt.columns.str.endswith(\"_r\")] /= 5 \n",
    "        au_dt, bs_dt = low_pass_filter_and_lag_slicing(bs_dt, au_dt, lag, 6)\n",
    "\n",
    "        #########Uncomment to see the AU26_r and JawOpen plot, and see how it goes\n",
    "        # fig, ax1 = plt.subplots()\n",
    "        # ax1.plot(au_dt[\"AU26_r\"], color=\"green\", label=\"AU26_r\")\n",
    "        # ax1.plot(bs_dt[\"JawOpen\"], color=\"blue\", label=\"JawOpen\")\n",
    "        # ax1.legend(loc=\"upper right\")\n",
    "        # display(fig)\n",
    "        \n",
    "        #Add script id column at the start of the dataframe\n",
    "        script_n = [Path(au_path).name.rsplit('!', 1)[0]] * au_dt.shape[0]\n",
    "        script_name_col = pd.DataFrame(script_n, columns=[\"script_id\"])\n",
    "\n",
    "        list_of_df.append(pd.concat([script_name_col, au_dt, bs_dt], axis=1))\n",
    "\n",
    "    giga_dataset = pd.concat(list_of_df)\n",
    "    giga_dataset.to_csv(\"giga_dataset.csv\",  index=False)\n",
    "#create_au_bs_dataset()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############Plot all the au/bs over all the scripts of all participants##############################\n",
    "#############You need to choose au and bs (e.g: AU26_r and JawOpen)\n",
    "# for participant in list_participants_id_path:\n",
    "#     list_of_openface_csvs = [f.path for f in os.scandir(participant) if f.is_file and f.name.endswith('.csv')]\n",
    "#     for openface_csv in list_of_openface_csvs:\n",
    "#         blendshape_csv = find_corresponding_blendshape_csv(participant, openface_csv)\n",
    "#         for bs, au in blendshape_au_dictionary.items():\n",
    "#             plot_for_testing_purposes(blendshape_csv, openface_csv, au, bs)\n",
    "\n",
    "\n",
    "#################Determine which windows size to choose with savgol filter###############################\n",
    "    # fig, axs = plt.subplots(nrows=10, ncols=2, figsize=(25, 50))\n",
    "    # for i, ax in zip(range(1, 20), axs.flat):\n",
    "    #     y1= signal.savgol_filter(y1, i, 0)\n",
    "    #     y2 = signal.savgol_filter(y2, i, 0)\n",
    "    #     ax.plot(x1, y1, color=\"green\", label=ActionUnitName)\n",
    "    #     ax.plot(x2.dt.total_seconds(), y2, color=\"blue\", label=blendshapeName)\n",
    "    #     ax.legend(loc=\"upper right\")\n",
    "    #     ax.title.set_text(blendshape_file + \"\\nsavgol size: \"+ str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_JawOpen_with_lag_corrected():\n",
    "    lag_csv = pd.read_csv(path+ \"/AU26_r-JawOpen-lagMeasure.csv\" , sep=',')\n",
    "    all_bs_csvs = lag_csv[\"blendshape_scriptID\"].apply(find_all_bs_csvs).values.tolist()\n",
    "    lag_vals = lag_csv[\"max_lag\"].values.tolist()\n",
    "    #load all the csvs\n",
    "    all_participants = lag_csv[\"participant_id\"].drop_duplicates(keep=\"first\")\n",
    "    all_au_csvs = all_participants.apply(lambda x: [f.path for f in os.scandir(x) if f.is_file and f.name.endswith('.csv')]) #find all au csv for each folder named as the participants\n",
    "    all_au_csvs = all_au_csvs.explode().reset_index(drop=True).values.tolist() #explode the lists inside cells in rows\n",
    "\n",
    "    au_csvs = remove_outliers(all_au_csvs, lag_csv, 60)\n",
    "    bs_csvs = remove_outliers(all_bs_csvs, lag_csv, 60)\n",
    "    lag_vals_without_outliers = remove_outliers(lag_vals, lag_csv, 60)\n",
    "    for au_path, bs_path, lag in zip(au_csvs, bs_csvs, lag_vals_without_outliers):\n",
    "        bs_dt = pd.read_csv(bs_path, index_col=0) #index_col=0 removes the unnamed: 0 column\n",
    "        au_dt = pd.read_csv(au_path, sep=\", \", engine='python')\n",
    "        #drop unecessary columns\n",
    "        au_dt.drop([\"frame\", \"face_id\", \"timestamp\", \"confidence\", \"success\"], axis=1, inplace=True)\n",
    "        au_dt = au_dt.loc[:, ~au_dt.columns.str.endswith(\"_c\")]\n",
    "        bs_dt = bs_dt.drop([\"Timecode\", \"BlendShapeCount\"], axis=1).reset_index(drop=True) #index starts at 0 with reset_index(drop=True)\n",
    "        #divide by 5 to normalize au value between 0 and 1\n",
    "        au_dt.loc[:, au_dt.columns.str.endswith(\"_r\")] /= 5 \n",
    "        au_dt, bs_dt = low_pass_filter_and_lag_slicing(bs_dt, au_dt, lag, 6)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(au_dt[\"AU26_r\"], color=\"green\", label=\"AU 26\")\n",
    "        ax.plot(bs_dt[\"JawOpen\"], color=\"blue\", label=\"JawOpen\")\n",
    "        ax.legend(loc=\"upper right\")\n",
    "        bs_name = Path(bs_path).name.rsplit(\"_\", 1)[0]\n",
    "        ax.set_xlabel(\"frame over time\")\n",
    "        ax.title.set_text(bs_name + \"  lag corrected\")\n",
    "        plt.savefig(\"JawOpen_AU26_corrected/\" + bs_name + \".png\")\n",
    "plot_JawOpen_with_lag_corrected()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
